<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-29T05:24:28-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sangoh Kim</title><subtitle>Undergraduate student of Korea Advanced Institute of Science and Technology (KAIST)</subtitle><author><name>Sangoh Kim</name><email>tkddh1109 (at) kaist.ac.kr</email></author><entry><title type="html">[Paper Review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)</title><link href="http://localhost:4000/posts/2024/12/ViT-paper-review/" rel="alternate" type="text/html" title="[Paper Review] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)" /><published>2024-12-29T00:00:00-08:00</published><updated>2024-12-29T00:00:00-08:00</updated><id>http://localhost:4000/posts/2024/12/ViT-paper-review</id><content type="html" xml:base="http://localhost:4000/posts/2024/12/ViT-paper-review/"><![CDATA[<p>ViT의 구조를 간단히 다루어보고, 논문의 Experiments/Appendix 위주로 리뷰해보았습니다.</p>

<h1 id="motivation">Motivation</h1>

<p>NLP 도메인에서 Self-Attention 기반 모델 (e.g. Transformer)가 널리 활용되고 있었습니다.
특히 large-scale dataset에서 pre-training을 진행한 후에 downstream task로 transfer 하는 방식이 주로 사용됩니다.
그러나 CV 도메인에서는 CNN 기반 모델이 아직 주류를 이루고 있었습니다. 
Attention과 CNN을 결합하거나, 또는 완전히 Attention으로 대체한 연구가 있긴 했으나 후자의 경우 흔히 생각하는 attention 과는 다르고
아직 ResNet 기반 구조가 SOTA 인 상태입니다.</p>

<p>저자는 Transformer를 이미지에 적용하면 CV 도메인에서도 Transformer의 강력한 장점(receptive field, generalization, etc.)을 활용할 수 있을 것이라고 생각하여 ViT를 제안하게 되었습니다.</p>

<h1 id="key-idea">Key Idea</h1>

<p>아래는 ViT의 전체적인 구조입니다. 저자는 Transformer가 NLP 도메인에서 보여줬던 장점을 최대한 유지하기 위해,
Transformer의 기존 구조를 최대한 변경하지 않도록 하였습니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-ModelArchitecture.png" height="500px" width="800px" />
</p>

<h2 id="model-architecture">Model Architecture</h2>

<p>이미지를 sequence 형태로 표현하기 위해서 저자는 이미지를 non-overlapping patch로 나누는 방식을 선택하였습니다.
즉 $\mathbb{R}^{H \times W \times C}$ 의 이미지가 있을 때 이를 $\mathbb{R}^{N \times (P^2C)}$ 로 바꾸는 것입니다.</p>

<p>ViT 내부의 Transformer는 latent vector dimension을 D로 유지하므로 $\mathbb{R}^{P^2C}$ 를 $\mathbb{R}^D$ 로
매핑해야합니다. 이를 위해 flattened image patch는 Linear Projection of Flattened Patches layer를 거치게 됩니다.</p>

<p>이후에 image patch 별로 1D positional encoding vector가 결합됩니다. 논문에서는 2D positional encoding 등 보다 고차원적인 방법들과 
1D의 성능 차이가 그리 크지 않아 1D PE를 사용했다고 합니다. 이 과정에서 sequence의 첫 시작 부분에 새로운 learnable embedding이 추가됩니다.
이 embedding은 이후 Transformer에서 image patch와 함께 인코딩 되어 이미지의 전체적인 정보를 저장하게 되고, 최종 단계에서 classification에 활용됩니다.</p>

<p>PE를 거친 sequence는 Transformer를 거쳐 인코딩되고 이후 0번째 embedding이 MLP head에 주어져 image classification을 수행합니다.
MLP head는 GELU nonlinearity를 이용한 2개의 linear layer로 구성되어 있다고 합니다.</p>

<h3 id="hybrid-architecture">Hybrid Architecture</h3>

<p>처음에 input sequence를 생성할 때 raw image를 patch로 나누는 것이 아닌, CNN을 거친 feature map을 사용하는 방식으로 진행하는 
hybrid architecture도 사용이 가능하다고 합니다.</p>

<h3 id="inductive-bias">Inductive bias</h3>

<p>Inductive bias는 <em>모델이 unseen sample에 대한 prediction을 할 때 기반하는 모든 가정</em> 을 의미합니다.</p>

<p>예를 들면 CNN 계열 모델의 inductive bias는 아래와 같을 것입니다.</p>
<ul>
  <li>Input data는 2D structured data 일 것이다.</li>
  <li>Input data에 대한 convolution은 translation equivariant 할 것이다.</li>
  <li>Input data는 locality를 가질 것이다.</li>
</ul>

<p>위 가정에 기반하여 CNN 모델은 local area에 filter를 적용하여 이미지를 처리하게 됩니다.</p>

<p>Transformer의 inductive bias는 아래와 같을 것입니다.</p>
<ul>
  <li>Input data는 sequence 형태일 것이다.</li>
  <li>Input data 각각은 positional encoding이 진행되었을 것이다.</li>
  <li>Input data 간의 pairwise relation learning을 통해 패턴을 발견할 수 있을 것이다.</li>
</ul>

<p>Inductive bias가 많을 수록 모델이 직접 학습, 처리해야하는 부분이 줄어드므로 더 효율적인 모델을 설계할 수 있을 것입니다.
반면 inductive bias에 부합하는 상황에만 모델을 적용할 수 있으니 일반성은 떨어지게 됩니다.
Inductive bias가 적으면 모델이 직접 학습해야하는 것들이 늘어나므로 더 많은 데이터를 제공해주어야 합니다.
학습 비용 등은 더 들지만 모델의 일반성은 더 향상되는 것입니다.</p>

<p>Transformer는 CNN보다 더 일반적인 inductive bias를 가지기에 많은 학습 데이터를 필요로 하지만 다양한 도메인에 적용이 가능한 것입니다.
ViT도 Transformer에 기반하기에 많은 학습데이터를 제공해주어야 합니다.
ImageNet-1k 정도의 mid-size dataset으로 학습한 경우 CNN 계열 모델이, ImageNet-21k 정도의 large-scale dataset의 경우에는 ViT의 성능이 더 좋았다고 합니다.</p>

<h2 id="fine-tuning-and-higher-resolution">Fine-Tuning and Higher Resolution</h2>

<p>ViT를 finetuning 할 때는 일반적으로 이미지의 해상도를 높혀서 진행합니다. 이미지의 해상도가 커지면 동일한 patch size에서 자연스레 sequence의 길이도 
길어지게 됩니다. 이때 pre-training에 맞춰진 positional embedding vector의 개수가 부족해질 수 있습니다. 논문에서는 이를 2D interpolation으로 해결했다고 합니다.</p>

<h1 id="experiments">Experiments</h1>

<ul>
  <li>ViT, ResNet, Hybrid를 서로 비교하는 방식입니다.</li>
  <li>각 모델에 적절한 data requirement, 모델별 pre-training cost 등을 측정하였다고 합니다.</li>
</ul>

<h2 id="setup">Setup</h2>

<h3 id="datasets">Datasets</h3>

<p>이미지 및 데이터셋에 대한 전체적인 전처리는 <a href="https://arxiv.org/abs/1912.11370">BiT</a>의 방식을 적용했다고 합니다.</p>

<p>Pre-training에 사용된 데이터셋은 아래와 같습니다.</p>
<ul>
  <li>ILSVRC-2012 ImageNet (1k classes, 1.3M images)</li>
  <li>ImageNet-21k (21k classes, 14M images)</li>
  <li>JFT (18k classes, 303M images)</li>
</ul>

<p>Fine-tuning에 사용된 데이터셋은 아래와 같습니다.</p>
<ul>
  <li>ImageNet</li>
  <li>CIFAR-10/100</li>
  <li>Oxford IIIT-Pets</li>
  <li>Oxford Flowers-102</li>
</ul>

<p>Pre-training 데이터셋의 이미지 중에서 Fine-tuning 데이터셋의 test set에 있는 이미지는 data leakage를 방지하기 위해 제거했다고 합니다.</p>

<h3 id="model-variants">Model Variants</h3>

<p>ViT의 경우 아래와 같이 모델 종류를 정의했습니다. ViT-B/16 으로 표기된 경우 ViT Base 크기의 모델이며 patch size가 16 x 16 임을 의미합니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-Models.png" height="500px" width="800px" />
</p>

<p>CNN 기반의 모델의 경우 ResNet을 개량한 “ResNet (BiT)”를 사용했습니다. 변형된 부분은 아래와 같습니다.</p>
<ul>
  <li>Batch Normalization을 Group Normalization으로 변경</li>
  <li><a href="https://arxiv.org/abs/1903.10520">Standard Convolution</a></li>
  <li>위 변화가 transfer 되었을 때의 성능을 증가시켜준다고 합니다.</li>
</ul>

<h3 id="training--fine-tuning">Training &amp; Fine-tuning</h3>

<p>ViT 및 ResNet (BiT) 모든 모델을 Adam optimizer로 학습하였습니다. 세부 설정은 아래와 같습니다.</p>
<ul>
  <li>$\beta_1 = 0.9, \beta_2 = 0.999$</li>
  <li>Batch size = 4096</li>
  <li>Weight decay = 0.1</li>
  <li>Linear learning rate warmup &amp; decay</li>
</ul>

<p>ImageNet에서 pretrain 하고 ImageNet에서 다시 finetune 한 경우에는 이미지의 resolution이 finetuning 할 때 증가되었다고 합니다.</p>

<h3 id="metrics">Metrics</h3>

<p>Pretraining이 완료된 이후에, 모델의 downstream task에 대한 성능을 측정하기 위해 논문에서는 두 가지 평가지표를 사용하였습니다.</p>

<p>첫 번째는 Fine-tuning accuracy 입니다. 이 방법은 pretrained 된 모델을 finetuning한 후 accuracy를 측정하는 일반적인 방법입니다.</p>

<p>그러나, finetuning cost로 인해 위 방법으로 측정하기 어려운 경우 논문에서는 Few-shot accuracy를 사용했습니다.
Few-shot accuracy는 finetuning dataset의 training image 중 일부만 이용하여 모델의 성능을 측정하는 방식입니다.
Linear 5-shot accuracy를 구한다고 가정했을 경우, training set에서 class 별로 5개의 이미지를 샘플링하고 least square objective로
regression 모델을 학습합니다. label은 각각의 entry가 [-1, 1] 범위인 K-dimensional 벡터를 사용합니다.</p>

<h2 id="comparison-to-state-of-the-arts">Comparison to State of the Arts</h2>

<p>비교군이 되는 기존 CNN 기반 모델은 아래와 같습니다.</p>
<ul>
  <li>Noisy Student: ImageNet에서 SOTA이며 large EfficientNet 기반 모델</li>
  <li>BiT-L: Large ResNet 기반. ImageNet 이외 다른 벤치마크에서 SOTA</li>
</ul>

<p align="center">
 <img src="/images/paper-review-ViT-perf.png" height="500px" width="800px" />
</p>

<p>위 결과에 따르면 ViT는 BiT-L, Noisy Student 기반 모델과 image classification 성능이 비슷하거나 더 높은 것을 확인할 수 있습니다.
Pretraining -&gt; Finetuning이 아닌 일반적인 CNN 모델의 성능과 비교해보아도 우수한 모습을 보여줍니다.</p>

<p>특히나 주목해야할 부분은 TPUv3-core-days 지표입니다. TPUv3-core-days는 해당 모델을 pretraining 하는데에
며칠이 걸리는지 나타낸 지표입니다. 예를 들어 제가 TPUv3 8 core로 pretraining을 진행한다면, ViT-L/16을 ImageNet-21k에서 pretraining 
하는데에는 $\frac{0.23k}{8} \approx 29$ 일이 소요되는 것입니다. TPUv3가 A100 정도 GPU와 비슷한 성능이라고 하는데, 한 달도 정말 길지만 
그래도 ViT가 BiT-L, Noisy Student 보다는 pretraining cost가 확실히 적은 것을 알 수 있습니다.</p>

<p>결론적으로는 ImageNet-21k에서 ViT-L/16을 pretraining 하는 것이 가장 pretraining cost도 적고 좋은 downstream performance를 얻을 수 있는 
방법인 것 같습니다.</p>

<h2 id="pre-training-data-requirements">Pre-Training Data Requirements</h2>

<p>저자는 inductive bias의 차이를 극복하기 위해서는 dataset size가 어느정도로 커져야 하는 지를 두 가지 실험을 통해 알아보았습니다.</p>

<p>첫 번째는 pretraining dataset을 ImageNet, ImageNet-21k, JFT-300M으로 점점 키우면서 ImageNet에서의 정확도를 측정한 것입니다.
ImageNet에서 pretraining하고 ImageNet에서 finetuning하는 것이 이상해 보이실 수 있는데 이때 이미지의 resolution을 finetuning할 때에는 
더 증가시켰다고 합니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-Exp1.png" height="500px" width="800px" />
</p>

<p>위 그림에서 회색으로 색칠되어있는 부분이 BiT 모델의 성능 범위입니다 (아마 여러 종류의 BiT로 성능을 측정해본듯 합니다).
ImageNet과 같이 작은 dataset에서는 ViT의 성능이 BiT를 넘어서지 못하지만 데이터셋의 크기가 커질수록 점점 비슷해지고 결국 능가하는 것을 볼 수 있습니다.
ViT의 pretraining cost가 훨씬 저렴하니 결국 ViT를 사용하는 것이 더 이득인 것을 알 수 있습니다.</p>

<p>두 번째는 JFT-300M의 subset으로 pretraining한 후에 ImageNet에서 Linear 5-shot accuracy를 측정하는 것입니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-Exp2.png" height="500px" width="800px" />
</p>

<p>10M 정도로 pretraining size가 작을 때는 BiT가 더 성능이 좋은 것을 확인할 수 있습니다. ViT는 데이터셋의 크기가 너무 작아 
일반적인 패턴을 감지하지 못하고 주어진 10M개의 데이터셋에 오버피팅된 것으로 추정됩니다. 그러나 pretraining 데이터셋 크기가 증가할 수록
점점 ViT의 성능이 좋아지는 것을 확인할 수 있습니다.</p>

<h2 id="scaling-study">Scaling Study</h2>

<p>BiT, ViT에 대해서 pretraining cost에 대한 transfer accuracy를 측정했습니다. Pretraining은 JFT-300M에서 진행했다고 합니다.
Average-5는 transfer accuracy를 측정한 5개 데이터셋에서의 평균, ImageNet은 ImageNet에서의 transfer accuracy를 나타낸 것입니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-Scaling.png" height="500px" width="800px" />
</p>

<p>위 결과에서 아래의 결론을 얻을 수 있습니다.</p>
<ol>
  <li>유사한 성능을 내기까지의 pretraining cost는 ViT가 더 적다.
    <ul>
      <li>Average-5에서 95%의 transfer accuracy를 얻기까지의 pretraining cost를 비교해보면 ViT가 상대적으로 매우 적은 cost만을 필요로 합니다.</li>
    </ul>
  </li>
  <li>Pretraining cost가 증가할수록 Hybrid와 ViT의 차이는 적어진다.</li>
  <li>ViT는 pretraining cost 증가에 따라 성능이 증가하며 saturation을 확인할 수 없어 만약 더 거대한 사이즈로 pretraining하면 더 좋은 성능을 보일 가능성이 있다.</li>
</ol>

<h1 id="appendix">Appendix</h1>

<p>여러 appendix 내용 중 self supervised learning, computation cost 분석에 관한 부분을 정리해보았습니다.</p>

<h2 id="self-supervision">Self-Supervision</h2>

<p>BERT의 방식을 적용하여 masked patch prediction task로 학습했다고 합니다. 이미지 하나별로 전체 patch sequence 중에 
50%를 corrupt하는데, 그 중 80%는 learnable [MASK] embedding으로 대체하고 10%는 random patch embedding으로 대체하고 나머지 10%는 그대로 유지했다고 합니다.</p>

<p>이후에 sequence를 ViT를 통해 encoding하고, corrupted patch에 대해서 mean color를 예측하는 방식으로 self-supervision을 진행하게 됩니다.
Ground-truth mean color는 corrupted patch에 해당하는 original patch를 이용해서 구하게 됩니다.</p>

<p>Training의 상세 과정은 다음과 같습니다.</p>
<ul>
  <li>JFT 데이터셋에서 pretraining</li>
  <li>batch size는 4096, epoch은 14 =&gt; 대략 10000만번의 backpropagation이 진행</li>
  <li>Adam optimizer를 사용함
    <ul>
      <li>basic learning rate: 2 x 10e-4</li>
      <li>10k step까지는 warmup, 이후부터 cosine learning rate decay</li>
    </ul>
  </li>
  <li>Self-supervision
    <ol>
      <li>mean color prediction</li>
      <li>mean color &amp; 4x4 downsized patch prediction</li>
      <li>full patch prediction 이후 L2 distance 측정</li>
    </ol>
  </li>
  <li>corruption rate를 15%로 변경해서도 진행</li>
</ul>

<p>위와 같이 pretraining한 후에 few-shot accuracy를 측정해본 결과 3번을 제외하고는 거의 비슷한 성능을 보였습니다(ImageNet에서 ViT-B/16이 대략 80%). 또한 corruption rate는 50%일 때가 더 성능이 높았다고 합니다.</p>

<p>또한 저자는 self-supervision에서는 위와 같은 enormous pretraining, large-scale dataset이 필요하지 않다고 합니다. 
Transfer performance가 pretraining step이 100k를 넘어간 이후부터는 감소하는 모습을 보였고, ImageNet에서 pretraining을 해도 위와 비슷한 성능을 
얻을 수 있었다고 합니다.</p>

<h2 id="empirical-computation-costs">Empirical Computation Costs</h2>

<p>TPUv3에서 inference speed를 측정해보았다고 합니다.</p>

<p align="center">
 <img src="/images/paper-review-ViT-inf-perf.png" height="500px" width="800px" />
</p>

<p>왼쪽 그림은 다양한 input image size에 대해서 각각의 모델이 TPUv3 코어 하나에서 초당 처리할 수 있는 이미지가 몇개인지에 대해 나타낸 것입니다.
오른쪽 그림은 다양한 input image size에 대해서 각각의 모델이 코어 하나에서 최대로 가질 수 있는 batch size에 대해서 나타낸 것입니다.</p>

<p>두 측면에서 ViT가 time/memory-efficient한 것을 확인할 수 있습니다.</p>

<h1 id="conclusion">Conclusion</h1>

<p>처음에 ViT의 사전학습에 한 달이 걸린다는 것을 보고 정말 놀랐습니다. 그래서 앞으로는 Self-supervision과 같이 더 적은 데이터셋으로, 혹은 더 
효율적인 학습 방법을 제시한 연구가 있는지 알아볼 것 같습니다. 또한 ResNet과 같은 mid-size dataset에서 학습된 일반적인 CNN과 ViT, BiT 등을 비교하여
어떤 상황에는 어떤 모델을 사용해보는것이 좋을 지 정리해볼 예정입니다.</p>

<p>읽어주셔서 감사합니다 :)</p>]]></content><author><name>Sangoh Kim</name><email>tkddh1109 (at) kaist.ac.kr</email></author><category term="AI" /><summary type="html"><![CDATA[ViT의 구조를 간단히 다루어보고, 논문의 Experiments/Appendix 위주로 리뷰해보았습니다.]]></summary></entry><entry><title type="html">[Paper Review] Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World (CaCao)</title><link href="http://localhost:4000/posts/2024/12/CaCao-paper-review/" rel="alternate" type="text/html" title="[Paper Review] Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World (CaCao)" /><published>2024-12-26T00:00:00-08:00</published><updated>2024-12-26T00:00:00-08:00</updated><id>http://localhost:4000/posts/2024/12/CaCao-paper-review</id><content type="html" xml:base="http://localhost:4000/posts/2024/12/CaCao-paper-review/"><![CDATA[<p>BERT 기반 data augmentation을 이용하여 scene graph generation 분야의 predicate imbalance 문제를 개선한 연구입니다.</p>

<hr />

<h1 id="motivation">Motivation</h1>

<p>Scene Graph Generation 분야에서는 predicate imbalance 문제가 주요 연구 주제 중 하나입니다.
Predicate imbalance 는 Visual Genome 등의 SGG 학습 데이터셋에서, informative/fine-grained predicate을 포함한
triplet이 frequent/coarse-grained predicate을 포함한 triplet에 비해 적은 현상을 의미합니다.
아래 왼쪽 그래프에서 확인할 수 있듯이, on/has와 같은 coarse-grained predicate의 개수가
carrying/laying on과 같은 fine-grained predicate 보다 압도적으로 많습니다.</p>

<p align="center">
 <img src="/images/paper-review-CaCao-long-tail.png" height="500px" width="800px" />
</p>

<p>위와 같은 predicate의 long-tail distribution은, SGG 모델 성능 하락의 큰 원인이 됩니다.
Predicate imbalance로 인해, 모델이 대부분의 relationship을 on, has와 같은 coarse-grained predicate으로 예측해버리는 현상이 나타나는 것입니다.
위 오른쪽 그래프에서, 모델의 예측성능(Recall@K)이 fine-grained predicate에 대해서는 매우 낮은 것을 확인할 수 있습니다.</p>

<p>Predicate imbalance 문제를 해결하기 위해 여러 방법론이 제안되었습니다.
대표적으로 causality에 기반한 <a href="https://arxiv.org/abs/2002.11949">TDE</a>, reweighting에 기반한 <a href="https://arxiv.org/abs/1812.01880">VCTree</a>가 있습니다.</p>

<p>위와 같은 여러 선행연구에서 여러가지 방법으로 predicate imbalance에 보다 robust한 SGG 모델을 학습할 수 있음을 보였지만, 
본 논문에서는 기존 선행연구들의 한계점을 아래와 같이 지적하였습니다.</p>
<ul>
  <li>대부분이 hand-designed rule에 기반하여 unscalable하다.</li>
  <li>Source data, model architecture에 따라 hyperparameter의 세밀한 조정이 필요하다.</li>
  <li>Prior data distribution에 의존한다.</li>
</ul>

<p>이러한 한계점에서 벗어나기 위해, 저자는 pretrained language model (BERT)의 extensive knowledge 사용을 제안하였습니다.
Large-scale dataset에서 사전학습된 언어 모델이 informative relationship에 대한 정보를 잘 알고 있을 것이라는 가정에 기반한 것입니다.</p>

<p>이를 위해 BERT에 기반하여 기존 데이터셋에서 tail predicate이 포함된 triplet을 증강해주는 
<strong>C</strong>ross-mod<strong>a</strong>l predi<strong>Ca</strong>te b<strong>o</strong>osting (<strong>CaCao</strong>) data augmentation framework를 제안하였습니다.
CaCao를 통해 Visual Genome 등의 기존 데이터셋을 증강하고, 이를 SGG 모델의 학습에 이용하여 성능을 개선하는 방식입니다.</p>

<h1 id="key-idea">Key Idea</h1>

<p align="center">
 <img src="/images/paper-review-CaCao-framework.png" height="500px" width="500px" />
</p>

<p>CaCao framework는 내부적으로 BERT 기반의 Visually-Prompted Language Model을 가지고 있으며,
COCO 데이터셋의 image, triplet pair로 Visually-Prompted Language Model을 학습합니다.
이후 학습이 완료된 모델을 이용해 기존 데이터셋의 unlabeled object pair에 대한 labeling을 진행하여 데이터셋을 증강하게 됩니다.
Unlabeled object pair는 Visual Genome을 예로 들면 bounding box coordinate을 이용하여 object 간의 IoU를 계산한 후,
특정값 이상이며 아직 레이블링 되어있지 않은 조합을 의미합니다.</p>

<p>COCO 데이터셋의 경우 저자가 uninformative, frequent predicate을 제거하는 전처리를 적용하여
585 종류의 informative predicate을 가지고 있는 상태입니다. 따라서 이 데이터셋을 학습한 Visually-Prompted Language Model은
informative predicate을 예측할 수 있게 됩니다.
단, COCO 데이터셋과 Visual Genome 등의 SGG 데이터셋의 label은 다릅니다. 따라서 데이터셋 증강 시에는
모델이 COCO의 informative predicate으로 예측하면 이를 증강하는 데이터셋의 적절한 label과 mapping하게 됩니다.</p>

<h2 id="visually-prompted-language-model">Visually-Prompted Language Model</h2>

<p align="center">
 <img src="/images/paper-review-VPLM.png" height="700px" width="500px" />
</p>

<p>Visually-Prompted Language Model의 내부적인 구조는 위와 같습니다.
Pre-trained Language Model은 Frozen BERT를 의미하며, Image Encoder는 Frozen ViT를 이용합니다.
모델이 내부적으로 학습하는 것은 오직 Visual Transformation Layer 및 Textual prompt 입니다.</p>

<p>Visual Transformation Layer는 image, text 간의 modality gap을 해소하기 위해 도입되었습니다.
BERT는 원래 언어 정보만 인식할 수 있어, SGG에 필요한 visual feature를 직접 제공할 수 없습니다.
따라서 visual feature를 text modality로 변환시키는 Visual Transformation Layer를 도입하여
이미지 정보를 함께 프롬프트로 제공하게 됩니다. 
Visual Transformation Layer는 내부적으로 MultiheadAttention, Feed Forward로 구성된 하나의 self attention module로 구현되어 있습니다.</p>

<p>Textual propmt는 정확한 용도를 제가 이해하지는 못했지만(논문에서는 efficient text prompt engineering을 위해서 도입되었다고 합니다)
보다 visual information과 text가 잘 융합되도록 해주는 역할로 생각하고 넘어갔습니다. 구현 상에서는 learnable token 10개로 구성하였습니다.</p>

<p>Visually-Prompted Language Model의 inference 과정은</p>
<ol>
  <li>Image Encoder &amp; Visual Transformation Layer, PLM Embedding Layer가 각각 image, triplet (e.g. [apple][MASK][table])을 prompt로 인코딩합니다.</li>
  <li>이후 textual prompt와 concatenation을 진행하여 전체 prompt를 완성한 후 BERT에 전달합니다.</li>
  <li>BERT는 제공받은 prompt를 처리합니다.</li>
  <li>BERT의 마지막 layer의 embedding 값을 classifier (linear layer)에 전달하여 최종 logit을 계산합니다.</li>
  <li>Logit 값을 이용해서 [MASK] 자리에 알맞는 predicate을 예측합니다.</li>
</ol>

<p>이제 Visually-Propmted Language Model의 training에 대해서 다루어보겠습니다.
Training 과정에서 학습하는건 visual transformation layer, textual propmt 입니다.
모델이 image, triplet pair를 제공받으면, 위 inference 과정과 동일하게 동작하고 최종적으로 logit을 도출해 냅니다.
이후에 logit에 softmax를 적용한 후 변형된 cross entropy loss로 학습을 진행합니다.
하나의 (image, triplet) pair에 대해서 objective는 아래와 같이 정의됩니다.</p>

<h2 id="adaptive-semantic-cluster-loss-ascl">Adaptive Semantic Cluster Loss (ASCL)</h2>

\[-\min \Sigma^{N_p}_{i=1} \mathop{\mathbb{E}}_\epsilon[\phi(y_i) + \Sigma _{j \in C_i} \frac{\epsilon_{i, j}}{|C_i|} \phi(y_j)] \log \psi(y_i | X_i)\]

<p>여기서 $\psi(y_i | X_i)$ 는 input prompt $X_i$ 의 masked position에 대한 probability distribution 입니다. $\phi(y_i)$ 는
ground-truth label을 나타내는 one-hot vector를 의미합니다.</p>

\[\mathop{\mathbb{E}}_\epsilon[\phi(y_i) + \Sigma _{j \in C_i} \frac{\epsilon_{i, j}}{|C_i|} \phi(y_j)]\]

<p>일반적인 cross entropy loss의 경우 위 부분이 $\phi(y_i)$ 여야 하지만 CaCao에서는 이를 조금 변형하여 사용합니다.
Predicate은 그 종류가 매우 다양하기 때문에 철자가 서로 다른 predicate이라도, 문맥상 비슷한 의미를 가지고 있는 경우가 있습니다(논문에서는 이를 semantic co-reference라고 부릅니다).
그러나 위 objective에 $\phi(y_i)$ 만을 사용한다면 예를 들어 ‘on’이 정답인 triplet에 대해 ‘above’라는 예측을 하였을 경우,
어느정도 비슷함에도 아예 다른 예측을 하였을 때와 동일하게 penalize 됩니다. 그렇게 된다면, 모델의 학습이 어려워지고 augmentation에 필요한 diversity를 얻기에도 어려워집니다.</p>

<p>이를 개선하기 위해 CaCao의 학습에는 위와 같은 loss가 사용됩니다 (Adaptive Semantic Cluster Loss라고 논문에서 부릅니다).
Loss 계산 방식은 우선 COCO 데이터셋의 predicate 별로 embedding을 계산합니다. Embedding은 &lt;subject-predicate-object&gt; 형태의 triplet이 있을 때,
이를 BERT에 넣고 마지막 layer의 embedding을 가져옵니다. 특정 predicate이 포함된 모든 triplet에 대해 동일하게 embedding을 계산하고 이를 평균 내주면
해당 predicate의 임베딩이 되는 방식입니다.</p>

<p>이후 predicate embedding 전체에 대해 K-Means Clustering을 진행합니다. 클러스터의 개수는 논문의 구현 상에서는 39로 하였으나 해당 값으로 결정하게 된 
근거는 찾지 못하였습니다. 클러스터링 이후 특정 label $y_i$ 에 대해서 $y_i$ 와 동일한 클러스터 내부에 있는 레이블을 함께 고려해주도록 하는 것이 위 수식의 의미가 됩니다.
같은 클러스터에 있는 다른 label에는 $\frac{\epsilon_{i, j}}{|C_i|}$ 만큼의 가중치가 붙는데 $\epsilon_{i, j}$는 i, j predicate embedding 간의 
cosine similarity 값을 의미합니다.</p>

<p>저자는 위 방식을 통해, 모델이 최대한 다양한 informative predicate을 예측하도록 유도할 수 있다고 주장하였습니다. 이를 위해서, $\psi(y_i | X_i)$ 도 일반적인
softmax에서 조금 변형하였습니다.</p>

\[\psi(y_i | X_i) = \frac{\exp({z_i})}{\Sigma_{j=1}^{K} w_{i, j}\exp({z_j})} \ w_{i, j} = \delta \frac{z_j}{z_i} \frac{n_j}{n_i}\]

<p>$z_i, z_j$는 logit 값을 의미하며 $n_i, n_j$는 i, j predicate의 초기 개수는 의미합니다. 위의 변형된 softmax를 통해 모델이 특정 predicate 만 과도하게
생성하는 것을 방지할 수 있었다고 합니다.</p>

<h2 id="epic">Epic</h2>

<p>CaCao를 이용하면 데이터셋을 효과적으로 증강한 후에 <a href="https://arxiv.org/abs/1701.02426">IMP</a>, <a href="https://arxiv.org/abs/1711.06640v2">Motifs</a>
등의 SGG 모델의 학습에 이용할 수 있습니다. 논문에서는 더 나아가 Open-World Predicate SGG 모델 구조를 제안하였습니다. 
<strong>E</strong>ntangled cross-modal <strong>p</strong>rompt approach for open-world pred<strong>i</strong>cate s<strong>c</strong>ene graph generation (Epic)으로 명명되며 
구조는 아래와 같습니다.</p>

<p align="center">
 <img src="/images/paper-review-CaCao-epic.png" height="500px" width="800px" />
</p>

<h1 id="experiments">Experiments</h1>

<p align="center">
 <img src="/images/paper-review-CaCao-Exp.png" height="500px" width="800px" />
</p>

<p>주요 SGG 모델을 CaCao로 학습하였을 때 위와 같이 기존 방법들보다 성능을 향상 시킬 수 있었다고 합니다. 저자는 predicate imbalance에 robust하기 위해
R@K가 아닌 mR@K를 사용하였으며, tail predicate에 대한 성능을 보다 면밀하게 측정할 수 있도록 빈도수 하위 50% predicate에 대해서만 성능을 측정한
Tail-R@K도 사용하였습니다.</p>

<h1 id="conclusion">Conclusion</h1>

<p>Data augmentation을 기반으로 predicate imbalance 문제를 해결한 연구는 있었으나 open world knowledge를 이용한건 본 논문이 처음인듯 합니다.
보통 Open-World SGG 모델을 새로 구축하는 연구들은 꽤 있었는데 Open-World로 데이터셋을 증강하고 closed SGG에 적용하는 접근도 있다는 것을 알게 되었습니다.
추가로, 본 논문의 github에서 확인해보니 클러스터링이 제대로 된 것인지 조금 의문이 들기는 합니다. 클러스터 개수 K도 적절한 것인지, 원소가 하나만 있는 클러스터도 
있던데 괜찮은 것인지, K-Means 이외의 다른 방식은 없는지에 대한 고민을 해보면 좋을 것 같습니다.</p>

<p>읽어주셔서 감사합니다 :)</p>]]></content><author><name>Sangoh Kim</name><email>tkddh1109 (at) kaist.ac.kr</email></author><category term="AI" /><category term="Scene Graph Generation" /><category term="Multimodal Learning" /><category term="Compotisional Generalization" /><summary type="html"><![CDATA[BERT 기반 data augmentation을 이용하여 scene graph generation 분야의 predicate imbalance 문제를 개선한 연구입니다.]]></summary></entry><entry><title type="html">[AI] Imputation Process for AI</title><link href="http://localhost:4000/posts/2024/03/imputation-process/" rel="alternate" type="text/html" title="[AI] Imputation Process for AI" /><published>2024-03-11T00:00:00-07:00</published><updated>2024-03-11T00:00:00-07:00</updated><id>http://localhost:4000/posts/2024/03/imp-process-ai</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/imputation-process/"><![CDATA[<p>AI 대회에서 활용하기 위해 여러 imputation 기법들을 정리해보았습니다.</p>

<blockquote>
  <p>2024.03.10
LG-Aimers 4기에 참가하면서 결측치 처리에 대한 저만의 명확한 프로세스가 없다는 것을 깨달았습니다. 물론 다양한 데이터셋에 따라 적절히 여러 방식을 적용하는 것이 필요하지만 어느정도 체계를 잡고 싶은 생각에 결측치 처리 방법 및 프로세스를 정리하였습니다.</p>

</blockquote>

<h1 id="결측값의-유형">결측값의 유형</h1>

<blockquote>
  <p>데이터가 결측될 확률에 따라 아래와 같이 3가지로 분류합니다.
현실적으로 실제 상황에서 결측치를 아래 3가지 중 하나로 명확히 분류하는 것은 어려운 것 같습니다.</p>

</blockquote>

<h2 id="mcar-missing-completely-at-random">MCAR (Missing Completely At Random)</h2>

<p>데이터가 결측될 확률이 모든 경우에서 같을 때 MCAR로 분류합니다. 즉 다른 feature가 어떤 값을 가지든, 혹은 결측된 데이터의 값이 어떻든 결측될 확률에 영향을 주지 않는 경우를 의미합니다.</p>

<ul>
  <li>데이터를 기입하는 과정에서 실수로 누락시켰거나 전산 오류가 발생하여 기입되지 않은 경우가 MCAR에 해당합니다.</li>
  <li>결측 원인이 데이터셋의 다른 값들과 전혀 영향이 없기 때문에 다른 feature, 같은 feature 내의 관측된 값으로 imputation을 진행하거나 결측된 데이터를 제거해주시면 됩니다.</li>
  <li>가장 이상적이며 편리한 경우입니다.</li>
</ul>

<h2 id="mar-missing-at-random">MAR (Missing At Random)</h2>

<p>데이터가 결측될 확률이 같은 feature 내의 observed data 내에서 동일한 경우에 MAR로 분류합니다. 즉 다른 feature의 값에 따라 데이터가 결측될 확률이 높아지거나, 낮아질 수 있지만 결측된 값과는 관련이 없을 때를 의미합니다.</p>

<ul>
  <li>몸무게를 측정할 때 젊은 여성 군집에서 몸무게 결측값이 많이 나온 경우 MAR로 분류할 수 있습니다.</li>
</ul>

<h2 id="mnar-missing-not-at-random">MNAR (Missing Not At Random)</h2>

<p>MCAR, MAR이 아닌 경우에 MNAR 또는 NMAR이라고 합니다. 즉 데이터가 결측될 확률이 다른 feature의 값, 결측된 그 자신의 값 또는 데이터셋에 기록되지 않은 외부 요인의 영향을 받을 경우를 의미합니다.</p>

<ul>
  <li>흡연 여부를 선택할 때 실제 흡연자가 사회적 인식을 고려해 사실대로 응답하지 않고 응답란을 비워두는 경우가 예시가 될 수 있습니다</li>
</ul>

<h2 id="mar-mcar-mnar-인지-판단하기">MAR, MCAR, MNAR 인지 판단하기</h2>

<p>결측값이 MAR, MCAR, MNAR인지 구분해주는 명확한 방법은 없습니다. 우선 모든 결측값을 MNAR이라고 기본적으로 가정한 뒤, 데이터셋에 대해서 알고 있는 사전 정보, 데이터셋 수집 방법을 통해 MAR, MCAR을 구분해 나가야 합니다.</p>

<p>데이터의 결측여부를 레이블로 하는 Logistic Regression 모델을 fitting해보는 방식으로 insight를 얻을 수도 있습니다. 결측 여부와 관련이 있는 feature를 찾게 된다면 MAR일 가능성이 높고 그렇지 않다면 MCAR, MNAR 중에서 구분하는 방식으로 판단해볼 수 있습니다.</p>

<h2 id="mar-mcar-mnar에-따른-처리-방법">MAR, MCAR, MNAR에 따른 처리 방법</h2>

<h3 id="mcar인-경우">MCAR인 경우</h3>

<p>Complete case analysis (결측값이 없는 샘플만 남겨놓기) 또는 Simple Imputation, Multiple Imputation 등등 적절히 imputation 방법을 적용하면 됩니다.</p>

<h3 id="mar인-경우">MAR인 경우</h3>

<p>MCAR만틈 valid하지 않지만 complete case analysis도 적용 가능합니다. Simple Imputation도 적용가능하며 Multiple Imputation또한 valid 합니다.</p>

<h3 id="mnar인-경우">MNAR인 경우</h3>

<p>결측값이 발생한 외부적인 요인이 없는지 여부를 조사하고 결측값에 대한 모델링이 필요합니다. 그리고 모델링 후 Multiple Imputation을 적용해볼 수 있습니다. 그러나 MNAR 결측값은 사실상 데이터셋 내부의 정보로 제대로 처리할 수 없기 때문에 일반적으로 MNAR 결측값인 경우 MAR로 가정하고 처리하는 것이 좋을 것 같습니다.</p>

<h1 id="결측값-처리-방법">결측값 처리 방법</h1>

<aside>
💡 MCAR, MAR로 가정하였습니다.

</aside>

<h2 id="결측값의-비율-확인하기-공식적인-가이드라인이-아닙니다">결측값의 비율 확인하기 (공식적인 가이드라인이 아닙니다!)</h2>

<table>
  <thead>
    <tr>
      <th>결측 비율</th>
      <th>처리 방법</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>~10%</td>
      <td>제거하기 또는 여러 방법의 imputation 적용하기</td>
    </tr>
    <tr>
      <td>10% ~ 20%</td>
      <td>Hot deck (KNN Imputation 등등), Regression, Model-based method imputation</td>
    </tr>
    <tr>
      <td>20% ~ 50%</td>
      <td>Model-based method, Regression imputation</td>
    </tr>
    <tr>
      <td>50% ~</td>
      <td>해당 feature 제거하기</td>
    </tr>
  </tbody>
</table>

<h2 id="0-결측값-그대로-두기">0. 결측값 그대로 두기</h2>

<p>결측 비율이 그리 크지 않고 XGBoost, LightGBM과 같이 결측치를 자동으로 처리하는 기능을 제공하는 모델을 사용하는 경우 그대로 두는 것도 하나의 방법이 될 수 있습니다.</p>

<h2 id="1-결측값-제거하기">1. 결측값 제거하기</h2>

<p>데이터셋에 따라 편향, 분산을 고려하여 적절한 방법을 사용해야 합니다.</p>

<ul>
  <li>결측치가 너무 많은 feature 제거하기</li>
  <li>결측치가 있는 row 제거하기</li>
</ul>

<h2 id="2-imputation">2. Imputation</h2>

<h3 id="2-1-simple-imputation">2-1. Simple Imputation</h3>

<p>단일 측정값을 이용해서 결측값을 대체하는 방법입니다. 구현이 간단하고 Multiple Imputation에 비해 처리 시간이 빠르고 자원을 많이 요구하지 않는다는 장점이 있습니다. Imputation을 할 때 가장 기본적으로 적용시켜보는 방법이며 더 복잡한 Imputation이 필요할 때 Multiple Imputation을 사용합니다.</p>

<ul>
  <li>mean, median, most frequent imputation</li>
  <li>constant imputation
    <ul>
      <li>0, -1 로 채우기 등등</li>
    </ul>
  </li>
  <li>KNN imputation
    <ul>
      <li>sklearn KNNImputer 이용하기</li>
      <li>outlier에 민감하므로 이상치를 미리 확인해보는 작업이 필요할 것 같음</li>
    </ul>
  </li>
</ul>

<h3 id="2-2-multiple-imputation">2-2. Multiple Imputation</h3>

<aside>
💡 MICE 외에도 MVNI 기법이 있지만 우선은 MICE에 대해서 알아보고 나머지는 추후에 알아보도록 하겠습니다.

</aside>

<p>Simple Imputation을 여러번 반복하여 결측값을 채우는 방법. 일종의 앙상블이라고 생각하면 됩니다. 결측비율이 꽤 높거나, MAR, MNAR에 가깝다고 생각되는 feature에 대해 적절히 적용하면 좋을 것 같습니다.</p>

<p><img src="/docs/AI/Untitled.png" alt="[Stef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate Imputation by Chained Equations in R”. Journal of Statistical Software](https://www.jstatsoft.org/article/view/v045i03)" /></p>

<p><a href="https://www.jstatsoft.org/article/view/v045i03">Stef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate Imputation by Chained Equations in R”. Journal of Statistical Software</a></p>

<ul>
  <li>MICE의 절차
    <ol>
      <li>데이터셋 생성(imputed data): 결측치를 특정 strategy를 이용해서 모두 대체한 데이터셋을 m개 생성합니다.</li>
      <li>분석과 추정: 임의로 결측값을 대체한 데이터셋을 estimator로 분석합니다.</li>
      <li>추정치 합치기</li>
    </ol>
  </li>
  <li>IterativeImputer
    <ul>
      <li>scikit-learn에서 MICE에 영감을 받아 구현된 imputer</li>
      <li>MICE 방식을 거의 동일하게 적용할 수 있음</li>
    </ul>
  </li>
</ul>

<h3 id="2-3-deep-learning-datawig-이용하기">2-3. Deep Learning (DataWig) 이용하기</h3>

<p>AWSLAB에서 <a href="https://dl.acm.org/citation.cfm?id=3272005">Biessmann, Salinas et al. 2018</a>를 이용하여 구축한 imputation 패키지입니다.. 범주형 특성에도 적용이 가능하며 다른 방식에 비해 비교적 정확하다고 합니다.. 추후 캐글에서 사용하며 구체적인 방식을 알아보겠습니다.</p>]]></content><author><name>Sangoh Kim</name><email>tkddh1109 (at) kaist.ac.kr</email></author><category term="AI" /><category term="Imputation" /><summary type="html"><![CDATA[AI 대회에서 활용하기 위해 여러 imputation 기법들을 정리해보았습니다.]]></summary></entry></feed>