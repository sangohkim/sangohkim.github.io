<!DOCTYPE html><html lang="en-US"><head><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=Edge"><link rel="stylesheet" href="/assets/css/just-the-docs-default.css"><link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"><style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > a, .site-nav > ul.nav-list:first-child > li > ul > li > ul > li:not(:nth-child(2)) > a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(4) > ul > li:nth-child(1) > ul > li:nth-child(2) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(4) > button svg, .site-nav > ul.nav-list:first-child > li:nth-child(4) > ul > li:nth-child(1) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list, .site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(4) > ul.nav-list > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; }</style><script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico" type="image/x-icon"><title>Convex Optimization | Sang-oh Kim</title><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Convex Optimization" /><meta property="og:locale" content="en_US" /><meta name="description" content="Step by step technical blog of myself" /><meta property="og:description" content="Step by step technical blog of myself" /><link rel="canonical" href="http://localhost:4000/docs/lgaimers/module2/cvx-opt/" /><meta property="og:url" content="http://localhost:4000/docs/lgaimers/module2/cvx-opt/" /><meta property="og:site_name" content="Sang-oh Kim" /><meta property="og:type" content="website" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Convex Optimization" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"Step by step technical blog of myself","headline":"Convex Optimization","url":"http://localhost:4000/docs/lgaimers/module2/cvx-opt/"}</script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } }, tex2jax: { inlineMath: [ ['$', '$'] ], displayMath: [ ['$$', '$$'] ], processEscapes: true, } }); MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"><title>Link</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"><title>Menu</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"><title>Expand</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"><polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"><title id="svg-external-link-title">(external link)</title><path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"><title>Document</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"><path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"><title>Search</title><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <symbol id="svg-copy" viewBox="0 0 16 16"><title>Copy</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/><path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"><title>Copied</title><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"><path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/><path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg><div class="side-bar"><div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Sang-oh Kim </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button></div><nav aria-label="Main" id="site-nav" class="site-nav"><ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a><li class="nav-list-item"><a href="/about/" class="nav-list-link">About</a><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Flutter Tips! category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/fluttertips/fluttertips-menu/" class="nav-list-link">Flutter Tips!</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/fluttertips/setup-macos/" class="nav-list-link">MacOs Flutter 개발환경 구성</a></ul><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in LG Aimers category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/lgaimers/lgaimers-menu/" class="nav-list-link">LG Aimers</a><ul class="nav-list"><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Mathematics for ML category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/docs/lgaimers/module2/lgaimers-mod2/" class="nav-list-link">Mathematics for ML</a><ul class="nav-list"><li class="nav-list-item"> <a href="/docs/lgaimers/module2/matrix-decomposition/" class="nav-list-link">Matrix Decomposition</a><li class="nav-list-item"> <a href="/docs/lgaimers/module2/cvx-opt/" class="nav-list-link">Convex Optimization</a><li class="nav-list-item"> <a href="/docs/lgaimers/module2/pca/" class="nav-list-link">PCA</a></ul></ul></ul></nav><footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</footer></div><div class="main" id="top"><div id="main-header" class="main-header"><div class="search" role="search"><div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Sang-oh Kim" aria-label="Search Sang-oh Kim" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label></div><div id="search-results" class="search-results"></div></div></div><div class="main-content-wrap"><nav aria-label="Breadcrumb" class="breadcrumb-nav"><ol class="breadcrumb-nav-list"><li class="breadcrumb-nav-list-item"><a href="/docs/lgaimers/lgaimers-menu/">LG Aimers</a><li class="breadcrumb-nav-list-item"><a href="/docs/lgaimers/module2/lgaimers-mod2/">Mathematics for ML</a><li class="breadcrumb-nav-list-item"><span>Convex Optimization</span></ol></nav><div id="main-content" class="main-content"><main><h1 id="convex-optimization"> <a href="#convex-optimization" class="anchor-heading" aria-labelledby="convex-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex Optimization</h1><h2 id="optimization이-ml에서-중요한-이유"> <a href="#optimization이-ml에서-중요한-이유" class="anchor-heading" aria-labelledby="optimization이-ml에서-중요한-이유"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimization이 ML에서 중요한 이유</h2><p>ML모델을 training하는 과정이 보통 최적의 parameter를 찾아내는 문제이다. 즉 Optimization 문제가 되는 경우가 많음.</p><h2 id="optimization-problem의-분류"> <a href="#optimization-problem의-분류" class="anchor-heading" aria-labelledby="optimization-problem의-분류"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Optimization problem의 분류</h2><ol><li>constraints가 있는지에 따라<ul><li>Constrained optimization<li>Unconstrained optimzation</ul><li>objective function, constraints가 convex인지에 따라<ul><li>Convex optimization 인지 여부로 분류 가능</ul></ol><h2 id="unconstrained-optimization"> <a href="#unconstrained-optimization" class="anchor-heading" aria-labelledby="unconstrained-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Unconstrained optimization</h2><p>Unconstrained optimization problem인 경우 gradient descent를 바로 활용할 수 있다.</p><h3 id="gradient-descent의-개념"> <a href="#gradient-descent의-개념" class="anchor-heading" aria-labelledby="gradient-descent의-개념"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gradient Descent의 개념</h3><p>objective function이 $f(\mathbf{x})$이고 epoch당 step size (learning rate)가 $\gamma_k$일 때,</p><ol><li>step size의 값이 적절하고<li>방향 $\mathbf{d}_k$가 $\nabla f(\mathbf{x}_k) \cdot \mathbf{d}_k &lt; 0$</ol><p>인 경우 local optimum을 구할 수 있음이 보장됨.</p><ul><li>gradient vector는 각 point에서 가장 빠르게 증가하는 방향으로 흐르기 때문에 gradient vector의 반대 방향으로 $\mathbf{d}_k$를 설정<li>step마다 learning rate가 달라질 수 있음(learning rate schedule을 사용하는 경우)<ul><li>sklearn의 SGDClassifier에서 learning_rate를 점진적으로 감소시키는 등의 방법</ul></ul><h3 id="gradient-descent의-종류"> <a href="#gradient-descent의-종류" class="anchor-heading" aria-labelledby="gradient-descent의-종류"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Gradient Descent의 종류</h3><p>일반적으로 최적화시켜야 하는 loss function은 각각의 data point에 대한 loss function을 더한 것으로 정의되는 경우가 많다(예를 들면 Linear regression에서 loss function은 MSE를 이용해 정의됨). 따라서 gradient를 구할 때도 각각의 data point에 대한 gradient를 모두 더하여 전체 gradient를 구해야하는 경우가 많다.</p><h4 id="1-batchfull-gradient-descent"> <a href="#1-batchfull-gradient-descent" class="anchor-heading" aria-labelledby="1-batchfull-gradient-descent"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Batch(Full) Gradient Descent</h4><p>위에서 설명한 기본적인 방식을 그대로 구현하는 GD 알고리즘. $\mathbf{\theta}_{k+1} = \mathbf{\theta}_k - \gamma_k \sum^n_{i=1} \nabla f(\mathbf{x}^{(i)})$</p><ul><li>sklearn에서 제공하지 않음<li>장점<ul><li>안정적으로 $\theta^*$를 찾아갈 수 있다</ul><li>단점<ul><li>안정적인 만큼 local minimum에 빠질 수 있다<li>step 마다 모든 data point에 대한 gradient를 구해야 하므로 오래 걸린다</ul></ul><h4 id="2-stochastic-gradient-descent"> <a href="#2-stochastic-gradient-descent" class="anchor-heading" aria-labelledby="2-stochastic-gradient-descent"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Stochastic Gradient Descent</h4><p>step 마다 전체 data point 중 하나를 랜덤으로 선택한 뒤 그 data point의 gradient만 사용하여 파라미터를 최적화시키는 방식. data point의 개수를 m이라고 할 때, 한 epoch마다 랜덤으로 data point를 선택하여 파라미터를 업데이트 하는 과정을 m번 반복함. $\mathbf{\theta}_{k+1} = \mathbf{\theta}_k - \gamma_k \nabla f(\mathbf{x}^{(i)})$</p><ul><li>장점<ul><li>한 step당 계산 시간이 줄어드므로 더 빠르게 global optimum에 도달할 수 있음<li>한번에 하나의 data point만 필요하므로 아주 큰 dataset에도 적용이 가능</ul><li>단점<ul><li>무작위성으로 인해 global optimum을 벗어날 확률마저 높아짐</ul><li>learning rate schedule<ul><li>위에서 언급한 SGD의 단점을 극복하기 위한 방법<li>learning rate를 점진적으로 감소시켜 global optimum에 안정적으로 도달할 수 있게 해주는 방식</ul><li>SGD를 적용하기 위한 조건<ul><li>dataset이 IID condition을 만족해야 함<ul><li>IID condition을 갖추어야만 global optimum을 향해 간다는 것이 보장됨<li>모든 data point가 서로 독립적이며(영향을 주지 않고) 같은 분포를 가져야 함<li>epoch 시작 또는 끝에 data point는 shuffle하거나 step 마다 data point를 랜덤으로 선택하는 방식을 사용하면 보장됨</ul><li>feature의 scale이 동일해야 함<ul><li>scale이 동일하지 않다면 global optimum에 도달하는 시간이 더 길어짐<li>sklearn의 StandardScaler() 이용하기</ul></ul><li>sklearn에서 SGD 사용하기<ul><li>SGDClassifier: default learning rate schedule 방식은 $\gamma_k = \frac{1}{alpha(t_0 + k)}$<li>SGDRegressor: default learning rate schedule 방식은 $\gamma_k = \frac{eta0}{k^{power_t}}$</ul></ul><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'''
적절한 dataset X, y가 주어진 경우
'''
import numpy as np
from sklearn.linear_model import SGDRegressor

sgd_reg = SGDRegressor()
sgd_reg.fit(X, y.ravel())
</code></pre></div></div><h4 id="3-mini-batch-gradient-descent"> <a href="#3-mini-batch-gradient-descent" class="anchor-heading" aria-labelledby="3-mini-batch-gradient-descent"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 3. Mini-batch Gradient Descent</h4><p>dataset을 mini-batch라고 부르는 작은 sample set으로 나누고 각각의 set에 대해서 FGD를 적용하는 방식. GPU 최적화를 잘 이용할 수 있다</p><ul><li>sklearn에서는 제공하지 않음</ul><h2 id="constrained-optimization"> <a href="#constrained-optimization" class="anchor-heading" aria-labelledby="constrained-optimization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Constrained Optimization</h2><p>Constrained Optimization인 경우 GD 등의 방법을 사용할 수 있게 unconstrained optimization problem으로 바꾼 후에 풀어야 함</p><h3 id="standard-constrained-optimization-problem"> <a href="#standard-constrained-optimization-problem" class="anchor-heading" aria-labelledby="standard-constrained-optimization-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Standard Constrained Optimization Problem</h3><p>optimization problem의 standard form: $minimize f(\mathbf{x})$ subject to $g_i(\mathbf{x}) \le 0$ for $i=1,…,m$ (Inequality constraints) $h_j(\mathbf{x}) = 0$ for $j=1,…,p$ (Equality constraints)</p><h3 id="problem-solving-via-lagrange-multipliers"> <a href="#problem-solving-via-lagrange-multipliers" class="anchor-heading" aria-labelledby="problem-solving-via-lagrange-multipliers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Problem solving via Lagrange Multipliers</h3><h4 id="duality-mentality"> <a href="#duality-mentality" class="anchor-heading" aria-labelledby="duality-mentality"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Duality Mentality</h4><p>Primal problem optimal value를 구할 수 있거나 또는 primal optimal value에 대한 bound를 제공할 수 있는 dual problem을 풀어서 primal problem의 optimal value를 구하자</p><h4 id="dual-problem-만들기"> <a href="#dual-problem-만들기" class="anchor-heading" aria-labelledby="dual-problem-만들기"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Dual Problem 만들기</h4><p>Lagrangian: $L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\nu}) = f(\mathbf{x}) + \sum^m_{i=1}\lambda_i g_i(\mathbf{x}) + \sum^p_{j=1}\nu_j h_j(\mathbf{x})$ ($\mathbf{\lambda}_i \ge 0$)</p><p>Lagrangian Dual Function: $D(\mathbf{\lambda}, \mathbf{\nu}) = inf_{\mathbf{x}}L(\mathbf{x}, \mathbf{\lambda}, \mathbf{\nu})$</p><ul><li>$\mathbf{\lambda}, \mathbf{\nu}$가 고정되었을 때 Lagrangian의 최솟값(infimum)<li>primal optimal value $p^*$의 lower bound가 된다<ul><li>primal optimal value는 결국 $f(\mathbf{x})$의 값인데 Lagrangian이 항상 $f(\mathbf{x})$보다 작음<li>$D(\mathbf{\lambda}, \mathbf{\nu}) \le p^*$가 항상 성립</ul></ul><p>Lagrangian Dual Problem: $max_{\mathbf{\lambda}, \mathbf{\nu}} D(\mathbf{\lambda}, \mathbf{\nu})$ subject to $\mathbf{\lambda} \succeq 0$</p><ul><li>$D(\mathbf{\lambda}, \mathbf{\nu}) \le p^*$가 항상 성립하므로 Lagrangian dual function의 최댓값을 구하면 그 값이 primal optimal value에 대한 best lower bound가 됨<li>Lagrangian Dual Problem은 항상 convex optimization<ul><li>GD 등의 방법을 이용해 풀 수 있음이 보장됨</ul></ul><p><strong>즉, constrained optimization problem의 경우 Dual Problem으로 바꿔서 푼다</strong></p><h3 id="duality"> <a href="#duality" class="anchor-heading" aria-labelledby="duality"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Duality</h3><p>primal problem과 dual problem 사이의 관계를 의미</p><p>primal problem optimal value: $p^*$, dual problem optimal value: $d^*$라고 하면</p><h4 id="weak-duality"> <a href="#weak-duality" class="anchor-heading" aria-labelledby="weak-duality"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Weak Duality</h4><p>$d^* \le p^*$</p><ul><li>primal, dual problem 사이에서 항상 성립하는 성질<li>$p^* - d^*$를 optimal duality gap이라고 함</ul><h4 id="strong-duality"> <a href="#strong-duality" class="anchor-heading" aria-labelledby="strong-duality"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Strong Duality</h4><p>$d^* = p^*$</p><ul><li>optimal duality gap이 0<li>대부분의 convex optimization problem에서 성립함(추가적인 몇가지 조건이 필요)</ul><h3 id="convex-optimization-1"> <a href="#convex-optimization-1" class="anchor-heading" aria-labelledby="convex-optimization-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex Optimization</h3><h4 id="standard-convex-optimization-problem"> <a href="#standard-convex-optimization-problem" class="anchor-heading" aria-labelledby="standard-convex-optimization-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Standard Convex optimization problem</h4><p>$minimize f(\mathbf{x})$ subject to $g_i(\mathbf{x}) \le 0$ for i = 1, …, m subject to $\mathbf{a}_j^T\mathbf{x}=\mathbf{b}_j$ for j = 1, …, p (f, g, h 모두 convex이며 정의역도 convex set이어야 함)</p><ul><li>upper bound inequality on “convex” function $\Rightarrow$ Constraint set이 convex<li>equality constraint는 affine하므로 convex</ul><h4 id="convex-set-이란"> <a href="#convex-set-이란" class="anchor-heading" aria-labelledby="convex-set-이란"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex Set 이란?</h4><ul><li>수학적 정의<ul><li>Set $C$가 $\forall \mathbf{x}, \mathbf{y} \in C, \theta \in [0, 1]$ 일 때 $\theta \mathbf{x} + (1-\theta) \mathbf{y} \in C$가 성립하면 convex set이라고 함<li>즉, set 내부의 임의의 두 점을 잇는 선분이 set 밖으로 나가지 않으면 convex set</ul><li>직관적으로 볼록한 모양의 set이라고 생각하면 됨</ul><h4 id="convex-function이란"> <a href="#convex-function이란" class="anchor-heading" aria-labelledby="convex-function이란"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex function이란?</h4><p>함수 $f: \mathbb{R}^n \rightarrow \mathbb{R}$의 domain이 convex set이고 domain 에 속하는 임의의 두 $\mathbf{x}, \mathbf{y}$가 $\theta \in [0, 1]$일 때 $f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) \le \theta f(\mathbf{x}) + (1-\theta) f(\mathbf{y})$를 만족하면 f를 convex funtion이라고 한다.</p><ul><li>$0 \lt \theta \lt 1$인 경우 strictly convex라고 함.<li>concave: -f가 convex인 경우<li>affine function: convex이면서 concave인 function<ul><li>linear function이 대표적인 예시</ul></ul><h4 id="convex-function의-특징"> <a href="#convex-function의-특징" class="anchor-heading" aria-labelledby="convex-function의-특징"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex function의 특징</h4><ul><li>local minimum이 곧 global minimum이다.<li>curvature(곡률)이 항상 증가한다(접선의 기울기가 항상 증가한다)</ul><h4 id="convex-function의-예시"> <a href="#convex-function의-예시" class="anchor-heading" aria-labelledby="convex-function의-예시"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex function의 예시</h4><p>$f(\mathbf{x}) = max{x_1, x_2, …, x_n}$ $f(\mathbf{x}) = log\sum^n_{i=1}e^{x_i}$</p><h4 id="convex-preserving-operation"> <a href="#convex-preserving-operation" class="anchor-heading" aria-labelledby="convex-preserving-operation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Convex-preserving operation</h4><ol><li>$f = \sum^n_{i=1}w_if_i$<li>$g(\mathbf{x}) = f(a\mathbf{x} + b)$<li>$f=max{f_1, f_2}$<li>기타 등등</ol><h4 id="lagrangian-dual-function이-convex-function인-이유"> <a href="#lagrangian-dual-function이-convex-function인-이유" class="anchor-heading" aria-labelledby="lagrangian-dual-function이-convex-function인-이유"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Lagrangian Dual Function이 Convex function인 이유</h4><ul><li>Point-wise supremum<ul><li>$f(\mathbf{x}, \mathbf{y})$가 y를 고정했을 때 $\mathbf{x}$에 대해서 convex이면 $g(\mathbf{x}) = inf_{\mathbf{y} \in \alpha} f(\mathbf{x}, \mathbf{y})$도 convex<li>$f(\mathbf{x}, \mathbf{y})$가 y를 고정했을 때 $\mathbf{x}$에 대해서 concave이면 $g(\mathbf{x}) = inf_{\mathbf{y} \in \alpha} f(\mathbf{x}, \mathbf{y})$도 concave</ul><li>Lagrangian dual function<ul><li>Lagrangian은 $\mathbf{x}$를 고정했을 때, linear함으로 concave하다(사실 affine이므로 convex, concave 모두 가능함)<li>따라서 Lagrangian dual function은 concave하다<li>Lagrangian dual problem은 concave function을 maximizing하는 것이므로 사실 convex function을 minimizing하는 것과 같다.<li><strong>결국 Dual problem은 convex optimization problem이다 $\Rightarrow$ 풀 수 있음이 보장된다.</strong></ul></ul><h3 id="kkt-condition-karush-kuhn-tucker-optimality-condition"> <a href="#kkt-condition-karush-kuhn-tucker-optimality-condition" class="anchor-heading" aria-labelledby="kkt-condition-karush-kuhn-tucker-optimality-condition"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> KKT Condition (Karush-Kuhn-Tucker Optimality condition)</h3><p>$g_i(\mathbf{x}) \le 0, h_j(\mathbf{x}) = 0, \lambda^*_i \ge 0, \lambda^*_ig_i(\mathbf{x})=0, \nabla L(\mathbf{x}^*, \mathbf{\lambda}^*, \mathbf{\nu}^*) = 0$ 일 때 KKT condition을 만족한다고 함.</p><h4 id="kkt-condition과-관련된-성질"> <a href="#kkt-condition과-관련된-성질" class="anchor-heading" aria-labelledby="kkt-condition과-관련된-성질"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> KKT Condition과 관련된 성질</h4><ul><li>strong duality가 만족되는 optimization problem의 primal, dual solution은 KKT condition을 만족<li>임의의 optimization problem의 primal, dual solution이 KKT condition을 만족하면 strong duality가 성립한다.</ul></main><hr><footer><p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2017-2020 Patrick Marsceill. Distributed by an <a href="https://github.com/just-the-docs/just-the-docs/tree/main/LICENSE.txt">MIT license.</a> <a href="https://www.netlify.com/">This site is powered by Netlify.</a></p><div class="d-flex mt-2"></div></footer></div></div><div class="search-overlay"></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@9.1.6/dist/mermaid.min.js"></script> <script> var config = {} ; mermaid.initialize(config); window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid')); </script>
